{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "learning_to_learn.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyPs0BKQmmhvLmOD2O7FyArP"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "TFz6phi-aSJQ"
      },
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.autograd import Variable\n",
        "from tqdm.notebook import tqdm\n",
        "import copy"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qvZkVoogaxCF"
      },
      "source": [
        "use_cuda = torch.cuda.is_available()\n",
        "\n",
        "def w(v):\n",
        "    if use_cuda:\n",
        "        # Returns a copy of the object in cuda memory \n",
        "        return v.cuda()\n",
        "\n",
        "    # Otherwise returns the object itself\n",
        "    return v"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "du2JjhKPa7eP"
      },
      "source": [
        "def detach_var(v):\n",
        "    var = w(v.data)\n",
        "    var.requires_grad_()\n",
        "    var.retain_grad()\n",
        "    return var"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "occtwIApbCRy"
      },
      "source": [
        "def train(opt_net, meta_opt, target_cls, target_to_opt, unroll, optim_iter, n_epochs, out_mul, training=True):\n",
        "    if training:\n",
        "        opt_net.train()\n",
        "    else:\n",
        "        opt_net.eval()\n",
        "        unroll = 1\n",
        "    \n",
        "    target = target_cls(training=training)\n",
        "    optimizee = w(target_to_opt())\n",
        "    n_params = 0\n",
        "\n",
        "    for p in optimizee.parameters():\n",
        "        n_params += int(np.prod(p.size()))\n",
        "\n",
        "    hidden_states = [w(torch.zeros(n_params, opt_net.hidden_sz)) for _ in range(2)]\n",
        "    cell_states = [w(torch.zeros(n_params, opt_net.hidden_sz)) for _ in range(2)]\n",
        "    all_losses_ever = []\n",
        "\n",
        "    if training:\n",
        "        meta_opt.zero_grad()\n",
        "\n",
        "    all_losses = None\n",
        "\n",
        "    for iteration in range(1, optim_iter + 1):\n",
        "        loss = optimizee(target)\n",
        "                    \n",
        "        if all_losses is None:\n",
        "            all_losses = loss\n",
        "        else:\n",
        "            all_losses += loss\n",
        "        \n",
        "        all_losses_ever.append(loss.data.cpu().numpy())\n",
        "        loss.backward(retain_graph=training)\n",
        "\n",
        "        offset = 0\n",
        "        result_params = {}\n",
        "        hidden_states2 = [w(torch.zeros(n_params, opt_net.hidden_sz)) for _ in range(2)]\n",
        "        cell_states2 = [w(torch.zeros(n_params, opt_net.hidden_sz)) for _ in range(2)]\n",
        "\n",
        "        for name, p in optimizee.all_named_parameters():\n",
        "            cur_sz = int(np.prod(p.size()))\n",
        "            gradients = detach_var(p.grad.view(cur_sz, 1))\n",
        "            updates, new_hidden, new_cell = opt_net(\n",
        "                gradients,\n",
        "                [h[offset:offset+cur_sz] for h in hidden_states],\n",
        "                [c[offset:offset+cur_sz] for c in cell_states]\n",
        "            )\n",
        "\n",
        "            for i in range(len(new_hidden)):\n",
        "                hidden_states2[i][offset:offset+cur_sz] = new_hidden[i]\n",
        "                cell_states2[i][offset:offset+cur_sz] = new_cell[i]\n",
        "                \n",
        "            result_params[name] = p + updates.view(*p.size()) * out_mul\n",
        "            result_params[name].retain_grad()\n",
        "            \n",
        "            offset += cur_sz\n",
        "            \n",
        "        if iteration % unroll == 0:\n",
        "            if training:\n",
        "                meta_opt.zero_grad()\n",
        "                all_losses.backward()\n",
        "                meta_opt.step()\n",
        "                \n",
        "            all_losses = None\n",
        "                        \n",
        "            optimizee = w(target_to_opt(**{k: detach_var(v) for k, v in result_params.items()}))\n",
        "            hidden_states = [detach_var(v) for v in hidden_states2]\n",
        "            cell_states = [detach_var(v) for v in cell_states2]\n",
        "            \n",
        "        else:\n",
        "            optimizee = w(target_to_opt(**result_params))\n",
        "            assert len(list(optimizee.all_named_parameters()))\n",
        "            hidden_states = hidden_states2\n",
        "            cell_states = cell_states2\n",
        "            \n",
        "    return all_losses_ever"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kaVXVGvlbJB3"
      },
      "source": [
        "def train_optimizer(target_cls, target_to_opt, preproc=False, unroll=20, optim_iter=100, n_epochs=20, n_tests=100, lr=0.001, out_mul=1.0):\n",
        "    opt_net = w(Optimizer(preproc=preproc))\n",
        "    meta_opt = optim.Adam(opt_net.parameters(), lr=lr)\n",
        "    \n",
        "    best_net = None\n",
        "    best_loss = 1e10\n",
        "    \n",
        "    for _ in tqdm(range(n_epochs), 'epochs'):\n",
        "        for _ in tqdm(range(20), 'iterations'):\n",
        "            train(opt_net, meta_opt, target_cls, target_to_opt, unroll, optim_iter, n_epochs, out_mul, training=True)\n",
        "        \n",
        "        loss = (np.mean([\n",
        "            np.sum(train(opt_net, meta_opt, target_cls, target_to_opt, unroll, optim_iter, n_epochs, out_mul, training=False))\n",
        "            for _ in tqdm(range(n_tests), 'tests')\n",
        "        ]))\n",
        "\n",
        "        print(loss)\n",
        "\n",
        "        if loss < best_loss:\n",
        "            print(best_loss, loss)\n",
        "            best_loss = loss\n",
        "            best_net = copy.deepcopy(opt_net.state_dict())\n",
        "            \n",
        "    return best_loss, best_net"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZUgIhVzLbShx"
      },
      "source": [
        "class Loss:\n",
        "    def __init__(self, **kwargs):\n",
        "        self.W = w(torch.randn(10, 10))\n",
        "        self.y = w(torch.randn(10))\n",
        "        \n",
        "    def get_loss(self, theta):\n",
        "        return torch.sum((self.W.matmul(theta) - self.y)**2)\n",
        "\n",
        "class ObjectiveFn(nn.Module):\n",
        "    def __init__(self, theta=None):\n",
        "        super().__init__()\n",
        "        if theta is None:\n",
        "            self.theta = nn.Parameter(torch.zeros(10))\n",
        "        else:\n",
        "            self.theta = theta\n",
        "        \n",
        "    def forward(self, target):\n",
        "        return target.get_loss(self.theta)\n",
        "    \n",
        "    def all_named_parameters(self):\n",
        "        return [('theta', self.theta)]"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "29zaez46bhNr"
      },
      "source": [
        "class Optimizer(nn.Module):\n",
        "    def __init__(self, preproc=False, hidden_sz=20, preproc_factor=10.0):\n",
        "        super().__init__()\n",
        "        self.hidden_sz = hidden_sz\n",
        "        if preproc:\n",
        "            self.recurs = nn.LSTMCell(2, hidden_sz)\n",
        "        else:\n",
        "            self.recurs = nn.LSTMCell(1, hidden_sz)\n",
        "        self.recurs2 = nn.LSTMCell(hidden_sz, hidden_sz)\n",
        "        self.output = nn.Linear(hidden_sz, 1)\n",
        "        self.preproc = preproc\n",
        "        self.preproc_factor = preproc_factor\n",
        "        self.preproc_threshold = np.exp(-preproc_factor)\n",
        "        \n",
        "    def forward(self, inp, hidden, cell):\n",
        "        if self.preproc:\n",
        "            inp = inp.data\n",
        "            inp2 = w(torch.zeros(inp.size()[0], 2))\n",
        "            keep_grads = (torch.abs(inp) >= self.preproc_threshold).squeeze()\n",
        "            inp2[:, 0][keep_grads] = (torch.log(torch.abs(inp[keep_grads]) + 1e-8) / self.preproc_factor).squeeze()\n",
        "            inp2[:, 1][keep_grads] = torch.sign(inp[keep_grads]).squeeze()\n",
        "            inp2[:, 0][~keep_grads] = -1\n",
        "            inp2[:, 1][~keep_grads] = (float(np.exp(self.preproc_factor)) * inp[~keep_grads]).squeeze()\n",
        "            inp = w(Variable(inp2))\n",
        "            \n",
        "        hidden0, cell0 = self.recurs(inp, (hidden[0], cell[0]))\n",
        "        hidden1, cell1 = self.recurs2(hidden0, (hidden[1], cell[1]))\n",
        "        return self.output(hidden1), (hidden0, hidden1), (cell0, cell1)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oL99DI6Vbu6P"
      },
      "source": [
        "loss, quad_optimizer = train_optimizer(Loss, ObjectiveFn, lr=0.003, n_epochs=10)\n",
        "print(loss)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}