{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "learning_to_learn.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMGEfanE1LCHs3F+2pCYnV3"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "TFz6phi-aSJQ"
      },
      "source": [
        "import numpy as np\r\n",
        "import torch\r\n",
        "import torch.nn as nn\r\n",
        "import torch.nn.functional as F\r\n",
        "import torch.autograd as autograd\r\n",
        "import torch.optim as optim\r\n",
        "from torch.autograd import Variable\r\n",
        "from tqdm.notebook import tqdm\r\n",
        "import copy"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qvZkVoogaxCF"
      },
      "source": [
        "use_cuda = torch.cuda.is_available()\r\n",
        "\r\n",
        "def w(v):\r\n",
        "    if use_cuda:\r\n",
        "        return v.cuda()\r\n",
        "    return v"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "du2JjhKPa7eP"
      },
      "source": [
        "def detach_var(v):\r\n",
        "    var = w(Variable(v.data, requires_grad=True))\r\n",
        "    var.retain_grad()\r\n",
        "    return var"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "occtwIApbCRy"
      },
      "source": [
        "def do_fit(opt_net, meta_opt, target_cls, target_to_opt, unroll, optim_it, n_epochs, out_mul, should_train=True):\r\n",
        "    if should_train:\r\n",
        "        opt_net.train()\r\n",
        "    else:\r\n",
        "        opt_net.eval()\r\n",
        "        unroll = 1\r\n",
        "    \r\n",
        "    target = target_cls(training=should_train)\r\n",
        "    optimizee = w(target_to_opt())\r\n",
        "    n_params = 0\r\n",
        "    for p in optimizee.parameters():\r\n",
        "        n_params += int(np.prod(p.size()))\r\n",
        "    hidden_states = [w(Variable(torch.zeros(n_params, opt_net.hidden_sz))) for _ in range(2)]\r\n",
        "    cell_states = [w(Variable(torch.zeros(n_params, opt_net.hidden_sz))) for _ in range(2)]\r\n",
        "    all_losses_ever = []\r\n",
        "    if should_train:\r\n",
        "        meta_opt.zero_grad()\r\n",
        "    all_losses = None\r\n",
        "    for iteration in range(1, optim_it + 1):\r\n",
        "        loss = optimizee(target)\r\n",
        "                    \r\n",
        "        if all_losses is None:\r\n",
        "            all_losses = loss\r\n",
        "        else:\r\n",
        "            all_losses += loss\r\n",
        "        \r\n",
        "        all_losses_ever.append(loss.data.cpu().numpy())\r\n",
        "        loss.backward(retain_graph=should_train)\r\n",
        "\r\n",
        "        offset = 0\r\n",
        "        result_params = {}\r\n",
        "        hidden_states2 = [w(Variable(torch.zeros(n_params, opt_net.hidden_sz))) for _ in range(2)]\r\n",
        "        cell_states2 = [w(Variable(torch.zeros(n_params, opt_net.hidden_sz))) for _ in range(2)]\r\n",
        "        for name, p in optimizee.all_named_parameters():\r\n",
        "            cur_sz = int(np.prod(p.size()))\r\n",
        "            gradients = detach_var(p.grad.view(cur_sz, 1))\r\n",
        "            updates, new_hidden, new_cell = opt_net(\r\n",
        "                gradients,\r\n",
        "                [h[offset:offset+cur_sz] for h in hidden_states],\r\n",
        "                [c[offset:offset+cur_sz] for c in cell_states]\r\n",
        "            )\r\n",
        "            for i in range(len(new_hidden)):\r\n",
        "                hidden_states2[i][offset:offset+cur_sz] = new_hidden[i]\r\n",
        "                cell_states2[i][offset:offset+cur_sz] = new_cell[i]\r\n",
        "            result_params[name] = p + updates.view(*p.size()) * out_mul\r\n",
        "            result_params[name].retain_grad()\r\n",
        "            \r\n",
        "            offset += cur_sz\r\n",
        "            \r\n",
        "        if iteration % unroll == 0:\r\n",
        "            if should_train:\r\n",
        "                meta_opt.zero_grad()\r\n",
        "                all_losses.backward()\r\n",
        "                meta_opt.step()\r\n",
        "                \r\n",
        "            all_losses = None\r\n",
        "                        \r\n",
        "            optimizee = w(target_to_opt(**{k: detach_var(v) for k, v in result_params.items()}))\r\n",
        "            hidden_states = [detach_var(v) for v in hidden_states2]\r\n",
        "            cell_states = [detach_var(v) for v in cell_states2]\r\n",
        "            \r\n",
        "        else:\r\n",
        "            optimizee = w(target_to_opt(**result_params))\r\n",
        "            assert len(list(optimizee.all_named_parameters()))\r\n",
        "            hidden_states = hidden_states2\r\n",
        "            cell_states = cell_states2\r\n",
        "            \r\n",
        "    return all_losses_ever"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kaVXVGvlbJB3"
      },
      "source": [
        "def fit_optimizer(target_cls, target_to_opt, preproc=False, unroll=20, optim_it=100, n_epochs=20, n_tests=100, lr=0.001, out_mul=1.0):\r\n",
        "    opt_net = w(Optimizer(preproc=preproc))\r\n",
        "    meta_opt = optim.Adam(opt_net.parameters(), lr=lr)\r\n",
        "    \r\n",
        "    best_net = None\r\n",
        "    best_loss = 1e10\r\n",
        "    \r\n",
        "    for _ in tqdm(range(n_epochs), 'epochs'):\r\n",
        "        for _ in tqdm(range(20), 'iterations'):\r\n",
        "            do_fit(opt_net, meta_opt, target_cls, target_to_opt, unroll, optim_it, n_epochs, out_mul, should_train=True)\r\n",
        "        \r\n",
        "        loss = (np.mean([\r\n",
        "            np.sum(do_fit(opt_net, meta_opt, target_cls, target_to_opt, unroll, optim_it, n_epochs, out_mul, should_train=False))\r\n",
        "            for _ in tqdm(range(n_tests), 'tests')\r\n",
        "        ]))\r\n",
        "        print(loss)\r\n",
        "        if loss < best_loss:\r\n",
        "            print(best_loss, loss)\r\n",
        "            best_loss = loss\r\n",
        "            best_net = copy.deepcopy(opt_net.state_dict())\r\n",
        "            \r\n",
        "    return best_loss, best_net"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZUgIhVzLbShx"
      },
      "source": [
        "class QuadraticLoss:\r\n",
        "    def __init__(self, **kwargs):\r\n",
        "        self.W = w(Variable(torch.randn(10, 10)))\r\n",
        "        self.y = w(Variable(torch.randn(10)))\r\n",
        "        \r\n",
        "    def get_loss(self, theta):\r\n",
        "        return torch.sum((self.W.matmul(theta) - self.y)**2)\r\n",
        "\r\n",
        "class QuadOptimizee(nn.Module):\r\n",
        "    def __init__(self, theta=None):\r\n",
        "        super().__init__()\r\n",
        "        if theta is None:\r\n",
        "            self.theta = nn.Parameter(torch.zeros(10))\r\n",
        "        else:\r\n",
        "            self.theta = theta\r\n",
        "        \r\n",
        "    def forward(self, target):\r\n",
        "        return target.get_loss(self.theta)\r\n",
        "    \r\n",
        "    def all_named_parameters(self):\r\n",
        "        return [('theta', self.theta)]"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "29zaez46bhNr"
      },
      "source": [
        "class Optimizer(nn.Module):\r\n",
        "    def __init__(self, preproc=False, hidden_sz=20, preproc_factor=10.0):\r\n",
        "        super().__init__()\r\n",
        "        self.hidden_sz = hidden_sz\r\n",
        "        if preproc:\r\n",
        "            self.recurs = nn.LSTMCell(2, hidden_sz)\r\n",
        "        else:\r\n",
        "            self.recurs = nn.LSTMCell(1, hidden_sz)\r\n",
        "        self.recurs2 = nn.LSTMCell(hidden_sz, hidden_sz)\r\n",
        "        self.output = nn.Linear(hidden_sz, 1)\r\n",
        "        self.preproc = preproc\r\n",
        "        self.preproc_factor = preproc_factor\r\n",
        "        self.preproc_threshold = np.exp(-preproc_factor)\r\n",
        "        \r\n",
        "    def forward(self, inp, hidden, cell):\r\n",
        "        if self.preproc:\r\n",
        "            inp = inp.data\r\n",
        "            inp2 = w(torch.zeros(inp.size()[0], 2))\r\n",
        "            keep_grads = (torch.abs(inp) >= self.preproc_threshold).squeeze()\r\n",
        "            inp2[:, 0][keep_grads] = (torch.log(torch.abs(inp[keep_grads]) + 1e-8) / self.preproc_factor).squeeze()\r\n",
        "            inp2[:, 1][keep_grads] = torch.sign(inp[keep_grads]).squeeze()\r\n",
        "            \r\n",
        "            inp2[:, 0][~keep_grads] = -1\r\n",
        "            inp2[:, 1][~keep_grads] = (float(np.exp(self.preproc_factor)) * inp[~keep_grads]).squeeze()\r\n",
        "            inp = w(Variable(inp2))\r\n",
        "        hidden0, cell0 = self.recurs(inp, (hidden[0], cell[0]))\r\n",
        "        hidden1, cell1 = self.recurs2(hidden0, (hidden[1], cell[1]))\r\n",
        "        return self.output(hidden1), (hidden0, hidden1), (cell0, cell1)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oL99DI6Vbu6P"
      },
      "source": [
        "loss, quad_optimizer = fit_optimizer(QuadraticLoss, QuadOptimizee, lr=0.003, n_epochs=10)\r\n",
        "print(loss)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}